# -*- coding: utf-8 -*-
"""FinalProjectSubmission

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19qj-cYeBg4ULxuZQ1fb1gdKUxjUmG5M1

**Deliverable 1**

Colab Notebook
"""

import pandas as pd
import numpy as np

from tqdm.auto import tqdm
from tqdm.notebook import tqdm, trange

import time
from time import sleep

import torch
from torchvision import datasets, transforms
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder
import torchvision.datasets as datasets
from torch.utils.data import DataLoader, TensorDataset
from torchvision.utils import make_grid
import cv2
import os
import pathlib
import imageio
from skimage.io import imread
from skimage.transform import resize
from skimage.color import rgb2gray
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import cv2
import glob
import seaborn as sns
import random
from sklearn.utils import resample

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device, torch.get_num_threads()

"""# DATA PREPARATION"""

## You have to install the Kaggle.json file from HW 4 for this notebook to work
! pip install kaggle
! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia
! unzip /content/chest-xray-pneumonia.zip

categories = ['NORMAL', 'PNEUMONIA']

base_dir = "/content/chest_xray/"

train_pneumonia_dir = base_dir+'train/PNEUMONIA/'
train_normal_dir=base_dir+'train/NORMAL/'

test_pneumonia_dir = base_dir+'test/PNEUMONIA/'
test_normal_dir = base_dir+'test/NORMAL/'

val_pneumonia_dir= base_dir+'val/PNEUMONIA/'
val_normal_dir= base_dir+'val/NORMAL/'

train_pn = [train_pneumonia_dir+"{}".format(i) for i in os.listdir(train_pneumonia_dir) ]
train_normal = [train_normal_dir+"{}".format(i) for i in os.listdir(train_normal_dir) ]

test_normal = [test_normal_dir+"{}".format(i) for i in os.listdir(test_normal_dir)]
test_pn = [test_pneumonia_dir+"{}".format(i) for i in os.listdir(test_pneumonia_dir)]

val_pn= [val_pneumonia_dir+"{}".format(i) for i in os.listdir(val_pneumonia_dir) ]
val_normal= [val_normal_dir+"{}".format(i) for i in os.listdir(val_normal_dir) ]

print ("Total images:",len(train_pn+train_normal+test_normal+test_pn+val_pn+val_normal))
print ("Total Pneumonia images:",len(train_pn+test_pn+val_pn))
print ("Total Normal images:",len(train_normal+test_normal+val_normal))

pn = train_pn + test_pn + val_pn
normal = train_normal + test_normal + val_normal

print(len(pn))
print(len(normal))

train_imgs = pn[:3418]+ normal[:1224] 
test_imgs = pn[3418:4059]+ normal[1224:1502]
val_imgs = pn[4059:] + normal[1502:]

import random

random.shuffle(train_imgs)
random.shuffle(test_imgs)
random.shuffle(val_imgs)

import cv2
img_size = 224

def preprocess_image(image_list):
    
    X = []
    y = [] 
    
    for image in image_list:

            img = cv2.imread(image,cv2.IMREAD_GRAYSCALE)    
            img=cv2.resize(img,(img_size,img_size),interpolation=cv2.INTER_CUBIC)
            img = np.dstack([img, img, img])
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            img = img.astype(np.float32)/255.
            img = img.flatten()

            X.append(img) 
            
            if 'NORMAL' in image:
                y.append(0)
            
            elif 'IM' in image:
                y.append(0)
            
            elif 'virus' or 'bacteria' in image:
                y.append(1)
            
    
    return X, y

X_train, y_train = preprocess_image(train_imgs)

X_train = np.asarray(X_train)
y_train = np.asarray(y_train)

X_val, y_val = preprocess_image(val_imgs)

X_val = np.asarray(X_val)
y_val = np.asarray(y_val)

X_test, y_test = preprocess_image(test_imgs)

X_test = np.asarray(X_test)
y_test = np.asarray(y_test)

"""# Exploratory Data Analysis """

base_path = '/content/chest_xray'
train_path = os.path.join(base_path, 'train/')
test_path = os.path.join(base_path,'test/')
val_path = os.path.join(base_path, 'val/')

#Creating dictionary for train dataset 
train_dataset_dict = {}
for (root, dirs, files) in os.walk(train_path):
  for dir in dirs:
    for files in os.walk((os.path.join(root, dir))):
      train_dataset_dict[dir] = len(files[2])

train_dataset_dict

#Creating dictionary for test dataset 
test_dataset_dict = {}
for (root, dirs, files) in os.walk(test_path):
  for dir in dirs:
    for files in os.walk((os.path.join(root, dir))):
      test_dataset_dict[dir] = len(files[2])

test_dataset_dict

#Creating dictionary for validation dataset 
val_dataset_dict = {}
for (root, dirs, files) in os.walk(val_path):
  for dir in dirs:
    for files in os.walk((os.path.join(root, dir))):
      val_dataset_dict[dir] = len(files[2])

val_dataset_dict

#Creating barplot of train dataset  
sns.set(rc={'figure.figsize':(12,8)})
p = sns.barplot(x = list(train_dataset_dict.keys()), y = list(train_dataset_dict.values()))
p.set_xlabel('Labels', fontsize = 12)
p.set_ylabel('Frequency', fontsize = 12)
p.set_title('Train dataset labels', fontsize = 20);

pneumonia = os.listdir('chest_xray/train/PNEUMONIA/')
pneumonia_dir = 'chest_xray/train/PNEUMONIA/'
normal = os.listdir('chest_xray/train/NORMAL/')
normal_dir = 'chest_xray/train/NORMAL/'

#show normal xray images
sample_idxs = [0, 10, 20, 30, 40]
for id in sample_idxs:
  plt.figure(figsize = (4,4))
  image = plt.imread(os.path.join(normal_dir, normal[id]))
  plt.imshow(image, cmap = 'gray')
  plt.title('Normal')

#show pneumonia xray images
sample_idxs = [0, 10, 20, 30, 40]
for id in sample_idxs:
  plt.figure(figsize = (4,4))
  image = plt.imread(os.path.join(pneumonia_dir, pneumonia[id]))
  plt.imshow(image, cmap = 'gray')
  plt.title('Pneumonia')

"""# PCA"""

#Visualize 2 components of PCA when applied to the training data
pca = PCA(n_components = 2)
x_train_pca = pca.fit_transform(X_train)
data = np.transpose(np.array(x_train_pca))

plt.figure(figsize=(12, 8))
plt.title('Top two prinicipal components\n', fontsize = 18)
plt.xlabel('\n Principal component 1', fontsize = 14)
plt.ylabel('Principal component 2\n', fontsize = 14);
plt.scatter(data[0][0:5000], data[1][0:5000])

pca = PCA(n_components = 200)
x_train_pca = pca.fit_transform(X_train)

explained_variance_ratios = pca.explained_variance_ratio_
cum_evr = np.cumsum(explained_variance_ratios)

plt.figure(figsize=(12, 8))
plt.plot(np.arange(1,201), cum_evr)
plt.title('PCA Explained Variance Ratios\n', fontsize = 18)
plt.xlabel('\n Number of components', fontsize = 14)
plt.ylabel('Cumulative explained variance ratios\n', fontsize = 14);

#50 components explains 80% of total variance in dataset 
pca = PCA(n_components = 50)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

"""EVALUATION METRICS:
1.   Accuracy
2.   Recall
3.   Precision
4.   F1-Score
5.   Confusion Matrix
"""

from sklearn.metrics import accuracy_score
from sklearn.metrics import balanced_accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn.metrics import f1_score #weighted for imbalanced dataset
from sklearn.metrics import confusion_matrix
import seaborn as sns

"""#SUPERVISED MACHINE LEARNING TECHNIQUES

1.   Decision Trees
2.   Random Forest
3.   KNN
4.   Fully Connected Networks

## Decision Tree
"""

from sklearn import tree

clf = tree.DecisionTreeClassifier()

clf = clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

#Results
print("Accuracy is: ", accuracy_score(y_test, y_pred))
print("Balanced Accuracy is: ", balanced_accuracy_score(y_test, y_pred))
print("Recall is: ",recall_score(y_test, y_pred))
print("Precision is: ",precision_score(y_test, y_pred))
print("F1 Weighted Score is: ",f1_score(y_test, y_pred, average = 'weighted'))
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix: ", cm)
tn, fp, fn, tp = cm.ravel()
print("True Negative: ", tn, "False Positive: ", fp, "False Negative: ", fn, "True Positive: ", tp)

confusion_matrix_df = pd.DataFrame(cm, columns = categories)
plt.subplots(figsize=(5,5))
ax = sns.heatmap(confusion_matrix_df,  annot=True,  fmt="d", cmap="YlGnBu")

"""## Random Forest"""

from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(max_depth=2, random_state=0)

clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

#Results
print("Accuracy is: ", accuracy_score(y_test, y_pred))
print("Balanced Accuracy is: ", balanced_accuracy_score(y_test, y_pred))
print("Recall is: ",recall_score(y_test, y_pred))
print("Precision is: ",precision_score(y_test, y_pred))
print("F1 Weighted Score is: ",f1_score(y_test, y_pred, average = 'weighted'))
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix: ", cm)
tn, fp, fn, tp = cm.ravel()
print("True Negative: ", tn, "False Positive: ", fp, "False Negative: ", fn, "True Positive: ", tp)

confusion_matrix_df = pd.DataFrame(cm, columns = categories)
plt.subplots(figsize=(5,5))
ax = sns.heatmap(confusion_matrix_df,  annot=True,  fmt="d", cmap="YlGnBu")

"""## KNN"""

from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors = 10)
knn_model.fit(X_train,y_train)

y_pred = knn_model.predict(X_test)

#Results
print("Accuracy is: ", accuracy_score(y_test, y_pred))
print("Balanced Accuracy is: ", balanced_accuracy_score(y_test, y_pred))
print("Recall is: ",recall_score(y_test, y_pred))
print("Precision is: ",precision_score(y_test, y_pred))
print("F1 Weighted Score is: ",f1_score(y_test, y_pred, average = 'weighted'))
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix: ", cm)
tn, fp, fn, tp = cm.ravel()
print("True Negative: ", tn, "False Positive: ", fp, "False Negative: ", fn, "True Positive: ", tp)

confusion_matrix_df = pd.DataFrame(cm, columns = categories)
plt.subplots(figsize=(5,5))
ax = sns.heatmap(confusion_matrix_df,  annot=True,  fmt="d", cmap="YlGnBu")

"""## Fully Connected Networks"""

import torch
import torchvision
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
import numpy as np
from PIL import Image

transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.CenterCrop(224),
    transforms.RandomRotation(20),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

base_dir = "/content/chest_xray/"

train_dir = base_dir+'train/'

test_dir = base_dir+'test/'

val_dir= base_dir+'val/'

train_dataset = torchvision.datasets.ImageFolder(train_dir, transform=transform)
val_dataset = torchvision.datasets.ImageFolder(val_dir, transform=transform)
test_dataset = torchvision.datasets.ImageFolder(test_dir, transform=transform)

dataset_sizes = {"train" : len(train_dataset), "val": len(val_dataset), "test": len(test_dataset)}

train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=256, num_workers = 2)
val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=256, num_workers = 2)
test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=256, num_workers = 2)

dataloaders = {"train": train_dataloader, "val":val_dataloader, "test":test_dataloader}

class FCN(nn.Module):
    def __init__(self):
        super(FCN, self).__init__()
        self.model = torchvision.models.resnet152(pretrained=True) #using pretrained ResNet model
        self.classifier = nn.Sequential(
        nn.Linear(self.model.fc.in_features,2), #binary classification
        nn.LogSoftmax(dim=1))
        for params in self.model.parameters():
            params.requires_grad = False #freezing the layers to make them non-trainable
        self.model.fc = self.classifier
    def forward(self, x):
        return self.model(x)

    def train_model(self, dataloaders, num_epochs):
        optimizer = optim.Adam(self.model.fc.parameters())
        criterion = nn.NLLLoss()
        best_acc = 0.0
        if torch.cuda.is_available():
            self.model = self.model.cuda()
        for epoch in range(num_epochs):
            print('Epoch: ', epoch)
            for phase in ['train', 'test']:
                if phase == 'train':
                    self.model.train() 
                else:
                    self.model.eval()
                running_loss = 0.0
                running_corrects = 0
                for inputs, labels in dataloaders[phase]:
                    if torch.cuda.is_available():
                        inputs = inputs.cuda()
                        labels = labels.cuda()
                    optimizer.zero_grad()
                    
                    with torch.set_grad_enabled(phase == 'train'):
                        outputs = self.model(inputs)
                        _, preds = torch.max(outputs, 1)

                        loss = criterion(outputs, labels)

                        if phase == 'train':
                            loss.backward()
                            optimizer.step()
                    running_loss += loss.item() * inputs.size(0)
                    running_corrects += torch.sum(preds == labels.data)

                epoch_loss = running_loss / dataset_sizes[phase]
                epoch_acc = running_corrects.double() / dataset_sizes[phase]
                print('{} Loss: {:.4f} Acc: {:.4f}'.format(
                    phase, epoch_loss, epoch_acc))
        return self.model

fcn_model = FCN()

#run 10 training epochs on our model
fcn_model_fit = fcn_model.train_model(dataloaders = dataloaders, num_epochs = 10)

"""# Addresing Class Imbalance by Upsampling or Downsampling"""

def fix_class_imabalance(major_data, minor_data, strategy):
  if strategy == "upsample":
    upsampled_data = resample(minor_data, replace = True, n_samples = len(major_data), random_state = 42)
    return upsampled_data
  
  if strategy == "downsample":
    downsampled_data = resample(major_data, replace = True, n_samples = len(minor_data), random_state = 42)
    return downsampled_data

pn_downsample = fix_class_imabalance(major_data = pn, minor_data = normal, strategy = "downsample")

print(len(pn_downsample))

normal_upsample = fix_class_imabalance(major_data = pn, minor_data = normal, strategy = "upsample")

print(len(normal_upsample))

train_imgs_down = pn_downsample[:1224]+ normal[:1224] 
test_imgs_down = pn_downsample[1224:1502]+ normal[1224:1502]
val_imgs_down = pn_downsample[1502:] + normal[1502:]

train_imgs_up = normal_upsample[:3418]+ normal[:3418] 
test_imgs_up = normal_upsample[3418:4059]+ normal[3418:4059]
val_imgs_up = normal_upsample[4059:] + normal[4059:]

import random
random.shuffle(train_imgs_down)
random.shuffle(test_imgs_down)
random.shuffle(val_imgs_down)
random.shuffle(train_imgs_up)
random.shuffle(test_imgs_up)
random.shuffle(val_imgs_up)
img_size = 224

X_train_d, y_train_d = preprocess_image(train_imgs_down)
X_val_d, y_val_d = preprocess_image(val_imgs_down)
X_test_d, y_test_d = preprocess_image(test_imgs_down)
X_train_u, y_train_u = preprocess_image(train_imgs_up)
X_val_u, y_val_u = preprocess_image(val_imgs_up)
X_test_u, y_test_u = preprocess_image(test_imgs_up)

X_train_d = np.asarray(X_train_d)
y_train_d = np.asarray(y_train_d)
X_val_d = np.asarray(X_val_d)
y_val_d = np.asarray(y_val_d)
X_test_d = np.asarray(X_test_d)
y_test_d = np.asarray(y_test_d)

X_train_u = np.asarray(X_train_u)
y_train_u = np.asarray(y_train_u)
X_val_u = np.asarray(X_val_u)
y_val_u = np.asarray(y_val_u)
X_test_u = np.asarray(X_test_u)
y_test_u = np.asarray(y_test_u)

from sklearn.neighbors import KNeighborsClassifier
knn_model_down = KNeighborsClassifier(n_neighbors = 10)
knn_model_down.fit(X_train_d,y_train_d)
y_pred = knn_model_down.predict(X_test_d)
#Results
down_acc = accuracy_score(y_test_d, y_pred)
down_bal_acc = balanced_accuracy_score(y_test_d, y_pred)
down_recall = recall_score(y_test_d, y_pred)
down_precision = precision_score(y_test_d, y_pred)
down_f1 = f1_score(y_test_d, y_pred, average = 'weighted')
print("Accuracy is: ", down_acc)
print("Balanced Accuracy is: ", down_bal_acc)
print("Recall is: ",down_recall)
print("Precision is: ",down_precision)
print("F1 Weighted Score is: ",down_f1)
cm = confusion_matrix(y_test_d, y_pred)
print("Confusion Matrix: ", cm)
tn, fp, fn, tp = cm.ravel()
print("True Negative: ", tn, "False Positive: ", fp, "False Negative: ", fn, "True Positive: ", tp)
confusion_matrix_df = pd.DataFrame(cm, columns = categories)
plt.subplots(figsize=(5,5))
ax = sns.heatmap(confusion_matrix_df,  annot=True,  fmt="d", cmap="YlGnBu")

knn_model_up = KNeighborsClassifier(n_neighbors = 10)
knn_model_up.fit(X_train_u,y_train_u)
y_pred = knn_model_up.predict(X_test_u)
#Results
up_acc = accuracy_score(y_test_u, y_pred)
up_bal_acc = balanced_accuracy_score(y_test_u, y_pred)
up_recall = recall_score(y_test_u, y_pred, zero_division=1)
up_precision = precision_score(y_test_u, y_pred, zero_division=1)
up_f1 = f1_score(y_test_u, y_pred, average = 'weighted')
print("Accuracy is: ", up_acc)
print("Balanced Accuracy is: ", up_bal_acc)
print("Recall is: ",up_recall)
print("Precision is: ",up_precision)
print("F1 Weighted Score is: ",up_f1)
cm = confusion_matrix(y_test_u, y_pred)
print("Confusion Matrix: ", cm)
if len(cm)==4:
  tn, fp, fn, tp = cm.ravel()
  print("True Negative: ", tn, "False Positive: ", fp, "False Negative: ", fn, "True Positive: ", tp)
  confusion_matrix_df = pd.DataFrame(cm, columns = categories)
  plt.subplots(figsize=(5,5))
  ax = sns.heatmap(confusion_matrix_df,  annot=True,  fmt="d", cmap="YlGnBu")

labels = ['Accuracy', 'Balanced Accuracy', 'Recall', 'Precision', 'F1-Score']
down = [down_acc, down_bal_acc, down_recall, down_precision, down_f1]
up = [up_acc, up_bal_acc, up_recall, up_precision, up_f1]

x = np.arange(len(labels))
width = 0.35

fig, ax = plt.subplots()
rects1 = ax.bar(x - width/2, down, width, label='Downsampling')
rects2 = ax.bar(x + width/2, up, width, label='Upsampling')
ax.set_ylabel('Performance')
ax.set_title('Comparison of Sampling Strategies')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()

"""# Hyperparameter Search using GridSearchCV"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

clf = RandomForestClassifier(random_state=42)

param_grid = { 
    'n_estimators': [50,100],
    'max_features': ['auto', 'sqrt'],
    'max_depth' : [4,8],
    'criterion' :['gini', 'entropy']
}

rf_grid = GridSearchCV(estimator=clf, param_grid=param_grid, cv= 3,verbose=3)
rf_grid.fit(X_train, y_train)

y_pred = rf_grid.predict(X_test)

#Results
print("Accuracy is: ", accuracy_score(y_test, y_pred))
print("Balanced Accuracy is: ", balanced_accuracy_score(y_test, y_pred))
print("Recall is: ",recall_score(y_test, y_pred))
print("Precision is: ",precision_score(y_test, y_pred))
print("F1 Weighted Score is: ",f1_score(y_test, y_pred, average = 'weighted'))
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix: ", cm)
tn, fp, fn, tp = cm.ravel()
print("True Negative: ", tn, "False Positive: ", fp, "False Negative: ", fn, "True Positive: ", tp)



"""# CNN"""

# Install dependencies 
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np
import os
import keras
import tensorflow
from keras.models import Sequential, Model
from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D
from keras import backend as K
from tensorflow.keras.optimizers import Adam,RMSprop,SGD
from keras.applications.densenet import DenseNet121
from keras.preprocessing.image import ImageDataGenerator, load_img
from sklearn.metrics import classification_report, confusion_matrix, plot_roc_curve, roc_curve, auc, f1_score, precision_score, recall_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA,  IncrementalPCA

cnn = Sequential()
cnn.add(Conv2D(filters=32, kernel_size=(3,3), activation="relu", padding="same",
                 input_shape=(64,64,1)))
cnn.add(BatchNormalization())
cnn.add(MaxPooling2D(pool_size=(2,2)))
cnn.add(Dropout(rate=0.3))
cnn.add(Conv2D(filters=64, kernel_size=(3,3), activation="relu", padding="same"))
cnn.add(BatchNormalization())
cnn.add(MaxPooling2D(pool_size=(2,2)))
cnn.add(Dropout(rate=0.3))
cnn.add(Flatten())
cnn.add(Dense(1024,activation="relu"))
cnn.add(BatchNormalization())
cnn.add(Dropout(rate=0.4))
cnn.add(Dense(2, activation="softmax"))
cnn.compile(Adam(lr=0.001), loss="categorical_crossentropy", metrics=["accuracy"])

cnn.summary()

train_clean = ImageDataGenerator()
size = (64,64)
training_data = train_clean.flow_from_directory('../content/chest_xray/train',
                                                 target_size = size,
                                                 color_mode="grayscale",
                                                 shuffle=True,
                                                 seed=42,
                                                 batch_size = 32)

validation_data = train_clean.flow_from_directory('../content/chest_xray/val/',
    target_size=size,
    color_mode="grayscale",
    shuffle=True,
    seed=42,
    batch_size=16)

test_data = train_clean.flow_from_directory('../content/chest_xray/test/',
    target_size=size,
    shuffle=False,
    color_mode="grayscale",
    batch_size=16)

cnn_model = cnn.fit(training_data,validation_data=validation_data,epochs=5)

plt.figure(figsize=(12, 12))

plt.subplot(2, 2, 1)
plt.plot(cnn.history['loss'], label='Training Loss')
plt.plot(cnn.history['val_loss'], label='Validation Loss')
plt.title('Training Loss Progression')
plt.xlabel('Epoch')
plt.legend(loc='upper left')
plt.subplot(2, 2, 2)
plt.plot(cnn.history['accuracy'], label='Training Accuracy')
plt.plot(cnn.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training Accuracy Progression')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

pred_cnn = cnn.predict(test_data)

df_predict_cnn = pd.DataFrame(pred_cnn)
df_predict_cnn["file"] = test_data.filenames
df_predict_cnn[1] = (df_predict_cnn[1]-df_predict_cnn[1].mean())/df_predict_cnn[1].std() + 0.5
df_predict_cnn["true_value"] = (df_predict_cnn["file"].str.contains("PNEUMONIA")).apply(int)
df_predict_cnn['pred_value'] = (df_predict_cnn[1]>0.5).apply(int)
df_predict_cnn

cnn_fpr, cnn_trp, cnn_threshold = roc_curve(df_predict_cnn["true_value"], df_predict_cnn[1])
auc_cnn = auc(cnn_fpr, cnn_trp)
plt.plot(cnn_fpr, cnn_trp, marker='.')
plt.title(label='ROC Curve (auc = %0.3f)' % auc_cnn)

confusion_matrix_cnn = pd.DataFrame(confusion_matrix(df_predict_cnn["true_value"], df_predict_cnn['pred_value']), columns = ['NORMAL', 'PNEUMONIA'])
plt.subplots(figsize=(5,5))
ax = sns.heatmap(confusion_matrix_cnn,  annot=True,  fmt="d", cmap="YlGnBu")

print("AUC is: ", auc_cnn)
print("F1 Weighted Score is: ", f1_score(df_predict_cnn["true_value"], df_predict_cnn['pred_value'], average = "weighted"))

test_accuracy = (df_predict_cnn['true_value']==df_predict_cnn['pred_value']).astype(int).mean()
test_accuracy

"""# Transfer Learning"""

# Install dependencies 
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import numpy as np
import os
import keras
import tensorflow
from keras.models import Sequential, Model
from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D
from keras import backend as K
from tensorflow.keras.optimizers import Adam,RMSprop,SGD
from keras.applications.densenet import DenseNet121
from keras.preprocessing.image import ImageDataGenerator, load_img
from sklearn.metrics import classification_report, confusion_matrix, plot_roc_curve, roc_curve, auc, f1_score, precision_score, recall_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA,  IncrementalPCA

base_densenet = DenseNet121(include_top=False, weights='imagenet')
pooling = GlobalAveragePooling2D()(base_densenet.output)
predictions = Dense(2, activation="softmax")(pooling)
densenet = Model(inputs=base_densenet.input, outputs=predictions)
densenet.compile(loss='categorical_crossentropy', 
              optimizer='adam', 
              metrics=['accuracy'])

clean = ImageDataGenerator()
size = (180,180)
training_data = clean.flow_from_directory('../content/chest_xray/train',
                                                 target_size = size,
                                                 shuffle=True,
                                                 seed=42,
                                                 batch_size = 32)

validation_data = clean.flow_from_directory('../content/chest_xray/val/',
    target_size=size,
    shuffle=True,
    seed=42,
    batch_size=16)

test_data = clean.flow_from_directory('../content/chest_xray/test/',
    target_size=size,
    shuffle=False,
    batch_size=16)

densenet_model = densenet.fit(training_data, epochs=5,
                              validation_data=validation_data)

plt.figure(figsize=(12, 12))
plt.subplot(2, 2, 1)
plt.plot(densenet_model.history['loss'], label='Training Loss')
plt.plot(densenet_model.history['val_loss'], label='Validation Loss')
plt.title('Training Loss Progression')
plt.xlabel('Epoch')
plt.legend(loc='upper left')
plt.subplot(2, 2, 2)
plt.plot(densenet_model.history['accuracy'], label='Training Accuracy')
plt.plot(densenet_model.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training Accuracy Progression')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

pred_densenet = densenet.predict(test_data)

df_predict_dense = pd.DataFrame(pred_densenet)
df_predict_dense["file"] = test_data.filenames
df_predict_dense[1] = (df_predict_dense[1]-df_predict_dense[1].mean()) + 0.5
df_predict_dense["true_value"] = (df_predict_dense["file"].str.contains("PNEUMONIA")).apply(int)
df_predict_dense['predict_value'] = (df_predict_dense[1]>0.5).apply(int)
df_predict_dense

confusion_matrix_cnn = pd.DataFrame(confusion_matrix(df_predict_dense["true_value"], df_predict_dense['predict_value']), columns = ['NORMAL', 'PNEUMONIA'])
plt.subplots(figsize=(5,5))
ax = sns.heatmap(confusion_matrix_cnn,  annot=True,  fmt="d", cmap="YlGnBu")

dn_fpr, dn_tpr, dn_threshold = roc_curve(df_predict_dense["true_value"], df_predict_dense[1])
auc_dn = auc(dn_fpr, dn_tpr)
plt.plot(dn_fpr, dn_tpr, marker='.')
plt.title(label='ROC Curve (auc = %0.3f)' % auc_dn)

print("AUC is: ", auc_dn)
print("F1 Weighted Score is: ", f1_score(df_predict_dense["true_value"], df_predict_dense['predict_value'], average = "weighted"))

dn_test_accuracy = (df_predict_dense['true_value']==df_predict_dense['predict_value']).astype(int).mean()
dn_test_accuracy

"""**Deliverable 2**

Link to MEDIUM article: https://medium.com/@datascientist98/detecting-the-presence-of-pneumonia-using-machine-learning-26c2161448a7
"""